# Moneycontrol News Scraper

Web scraper untuk mengekstrak berita dari Moneycontrol.com bagian Markets (https://www.moneycontrol.com/news/business/markets/)

## ğŸ“ Struktur Project

```
portofolio-scraper/
â”œâ”€â”€ scrapers/                    # Core scraper modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ crawl4ai_scraper.py      # Scraper dengan Crawl4AI (RECOMMENDED)
â”‚   â”œâ”€â”€ playwright_scraper.py    # Scraper dengan Playwright
â”‚   â”œâ”€â”€ requests_scraper.py      # Scraper dengan Requests (basic)
â”‚   â””â”€â”€ auto_pages_scraper.py    # Enhanced scraper dengan auto-detect pages
â”œâ”€â”€ examples/                    # Example & template scripts
â”‚   â”œâ”€â”€ custom_scraper.py        # Template untuk custom website
â”‚   â””â”€â”€ json_output_examples.py  # Contoh berbagai format JSON output
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ SCRAPING_GUIDE.md        # Panduan lengkap web scraping
â”‚   â””â”€â”€ ERROR_HANDLING_GUIDE.md  # Troubleshooting & error handling
â”œâ”€â”€ run_crawl4ai.py             # Quick run script (Crawl4AI)
â”œâ”€â”€ run_playwright.py           # Quick run script (Playwright)
â”œâ”€â”€ run_requests.py             # Quick run script (Requests)
â”œâ”€â”€ upload_to_mongodb.py        # Upload JSON data ke MongoDB
â”œâ”€â”€ config.py                   # Konfigurasi settings
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ .env.example                # Contoh konfigurasi environment variables
â”œâ”€â”€ README.md                   # Dokumentasi utama
â””â”€â”€ .gitignore                  # Git ignore rules
```

## âœ¨ Fitur

- âœ… **3 Mode Scraper**: Crawl4AI (powerful), Playwright (reliable), Requests (simple)
- âœ… **Auto Page Detection**: Otomatis detect total pages yang tersedia
- âœ… **Detail Extraction**: Fetch date & author dari detail page
- âœ… **Concurrency Control**: Limit concurrent requests untuk stabilitas
- âœ… **Error Handling**: Retry mechanism dengan exponential backoff
- âœ… **Multiple Export**: JSON, CSV, dan Excel
- âœ… **MongoDB Integration**: Upload otomatis ke MongoDB dengan deduplication
- âœ… **Comprehensive Logging**: Track semua aktivitas scraping

## ğŸš€ Quick Start

### 1. Clone & Setup

```bash
git clone <repository-url>
cd portofolio-scraper
```

### 2. Install Dependencies

```bash
# Buat virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# atau
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Install Playwright browsers (untuk Crawl4AI & Playwright)
playwright install chromium
```

### 3. Run Scraper

**Option 1: Crawl4AI (RECOMMENDED ğŸš€)**
```bash
python run_crawl4ai.py
```

**Option 2: Playwright**
```bash
python run_playwright.py
```

**Option 3: Requests (Simple)**
```bash
python run_requests.py
```

## ğŸ“Š Output

Scraper akan menghasilkan:
```
moneycontrol_news_crawl4ai.json    # Data dalam format JSON
moneycontrol_news_crawl4ai.csv     # Data dalam format CSV
scraper_crawl4ai.log               # Log file untuk debugging
```

### Contoh Output JSON

```json
[
  {
    "title": "Powell says tariffs adding some pressure to inflation...",
    "url": "https://www.moneycontrol.com/news/business/markets/...",
    "summary": "Federal Reserve Chair Jerome Powell said tariffs...",
    "image_url": "https://images.moneycontrol.com/...",
    "date": "November 07, 2025",
    "author": "Moneycontrol News",
    "scraped_at": "2025-11-07T08:30:00.123456"
  }
]
```

## ğŸ’¾ Upload ke MongoDB

Upload hasil scraping ke MongoDB dengan mudah:

### Setup

```bash
# Install pymongo (sudah ada di requirements.txt)
pip install pymongo

# Copy .env.example ke .env
cp .env.example .env

# Edit .env dan isi dengan MongoDB credentials Anda
nano .env  # atau text editor lainnya
```

### Konfigurasi

Edit `.env` file:

```bash
MONGODB_CONNECTION_STRING=mongodb://localhost:27017/
MONGODB_DATABASE_NAME=moneycontrol_db
MONGODB_COLLECTION_NAME=news_articles
```

Atau edit langsung di `upload_to_mongodb.py`:

```python
MONGODB_CONNECTION_STRING = "mongodb://localhost:27017/"
DATABASE_NAME = "moneycontrol_db"
COLLECTION_NAME = "news_articles"
```

### Upload Data

```bash
python upload_to_mongodb.py
```

**Fitur Upload:**
- Auto-deduplication berdasarkan URL
- Upsert mode (update existing + insert new)
- Bulk operations untuk performa tinggi
- Progress tracking & logging
- Error handling yang robust
- Auto-indexing untuk query performance

## ğŸ›ï¸ Konfigurasi

Edit `config.py` untuk customize:

```python
NUM_PAGES = 3              # Jumlah halaman yang akan di-scrape
DELAY_BETWEEN_PAGES = 2.0  # Delay antar page (detik)
OUTPUT_JSON = True         # Export ke JSON
OUTPUT_CSV = True          # Export ke CSV
```

Atau configure di code:

```python
from scrapers import MoneyControlCrawl4AIScraper

# Custom configuration
scraper = MoneyControlCrawl4AIScraper(
    fetch_details=True,      # Fetch date & author dari detail page
    max_concurrent=5         # Max 5 concurrent requests (adjust sesuai network)
)

# Scrape dengan custom settings
articles = await scraper.scrape_multiple_pages(
    num_pages=5,            # Scrape 5 pages
    delay=2.0               # 2 detik delay antar page
)
```

## ğŸ“š Mode Scraper

### ğŸš€ **Crawl4AI Scraper** (RECOMMENDED)

**Keunggulan:**
- Built khusus untuk AI/LLM extraction
- Otomatis handle JavaScript
- Async untuk performa tinggi
- Smart content extraction
- **Concurrency control** untuk stabilitas

**Penggunaan:**
```bash
python run_crawl4ai.py
```

**Konfigurasi:**
```python
scraper = MoneyControlCrawl4AIScraper(
    fetch_details=True,     # Fetch dari detail page
    max_concurrent=5        # Limit concurrent requests
)
```

---

### ğŸ­ **Playwright Scraper**

**Keunggulan:**
- Full browser automation
- Reliable dan stabil
- Async/await support
- Debugging yang mudah

**Penggunaan:**
```bash
python run_playwright.py
```

---

### ğŸ“¡ **Requests Scraper** (Basic)

**Keunggulan:**
- Paling ringan dan cepat
- Tidak perlu browser
- Cocok untuk static content

**Penggunaan:**
```bash
python run_requests.py
```

---

## ğŸ”§ Troubleshooting

### Error: Timeout

Jika sering terjadi timeout:

```python
# Kurangi concurrent requests
scraper = MoneyControlCrawl4AIScraper(max_concurrent=3)

# Tambah delay antar page
articles = await scraper.scrape_multiple_pages(num_pages=3, delay=3.0)
```

Baca: `docs/ERROR_HANDLING_GUIDE.md`

### Error: UnicodeEncodeError (Windows)

Sudah diperbaiki! Emoji diganti dengan text labels.

### Error: Playwright browser tidak tersedia

```bash
playwright install chromium
```

### Error: Module not found

```bash
pip install -r requirements.txt --upgrade
```

---

## ğŸ“– Dokumentasi Lengkap

- **`docs/SCRAPING_GUIDE.md`** - Panduan lengkap web scraping
  - Konsep dasar & flow
  - Cara inspect element
  - CSS selector cheat sheet
  - Template code berbagai use case
  - Debugging tips

- **`docs/ERROR_HANDLING_GUIDE.md`** - Troubleshooting guide
  - Root cause analysis
  - Concurrency limiting
  - Retry mechanism
  - Configuration options

## ğŸ“ Examples

### Custom Scraper untuk Website Lain

```bash
python examples/custom_scraper.py
```

Template yang bisa di-customize untuk scrape website apapun.

### Berbagai Format JSON Output

```bash
python examples/json_output_examples.py
```

Generate 8 format JSON berbeda:
- Standard (default)
- Compact (space-efficient)
- With metadata
- Grouped by date
- JSONL (JSON Lines)
- Custom fields
- API format

---

## ğŸ› ï¸ Development

### Struktur Data

Setiap artikel memiliki field:

| Field | Deskripsi | Source |
|-------|-----------|--------|
| `title` | Judul berita | List page |
| `url` | Link artikel lengkap | List page |
| `summary` | Ringkasan/excerpt | List page |
| `image_url` | URL gambar thumbnail | List page |
| `date` | Tanggal publikasi | Detail page |
| `author` | Nama penulis | Detail page |
| `scraped_at` | Timestamp scraping | Generated |

### Import Sebagai Module

```python
from scrapers import MoneyControlCrawl4AIScraper

# Initialize
scraper = MoneyControlCrawl4AIScraper(
    fetch_details=True,
    max_concurrent=5
)

# Scrape
articles = await scraper.scrape_multiple_pages(num_pages=3)

# Save
scraper.save_to_json(articles, 'my_output.json')
scraper.save_to_csv(articles, 'my_output.csv')
```

### Menambah Field Baru

Edit fungsi `extract_article_data()` di scraper:

```python
def extract_article_data(self, article_element):
    # ... existing code ...

    # Tambah field baru
    category_elem = article_element.find('span', class_='category')
    article_data['category'] = category_elem.get_text(strip=True) if category_elem else ''

    return article_data
```

---

## âš™ï¸ Best Practices

1. **Rate Limiting**: Gunakan delay minimal 2 detik antar request
2. **Concurrency**: Set `max_concurrent=3-5` untuk stabilitas
3. **Error Handling**: Check logs untuk troubleshooting
4. **Respect ToS**: Gunakan data secara bertanggung jawab
5. **Off-Peak Hours**: Scrape di jam sepi untuk performa lebih baik

---

## ğŸ“ˆ Performance Tips

### Untuk Speed:
```python
# Increase concurrent (hati-hati!)
scraper = MoneyControlCrawl4AIScraper(max_concurrent=8)
```

### Untuk Stability:
```python
# Decrease concurrent & add delay
scraper = MoneyControlCrawl4AIScraper(max_concurrent=3)
articles = await scraper.scrape_multiple_pages(num_pages=5, delay=3.0)
```

### Auto-Detect All Pages:
```python
from scrapers import EnhancedMoneyControlScraper

scraper = EnhancedMoneyControlScraper()
articles = await scraper.scrape_all_pages()  # Otomatis detect & scrape semua
```

---

## ğŸ” Requirements

- Python 3.8+
- requests
- beautifulsoup4
- pandas
- crawl4ai (untuk Crawl4AI scraper - RECOMMENDED)
- playwright (untuk Playwright dan Crawl4AI scraper)

---

## ğŸ“ License

MIT License

---

## âš ï¸ Disclaimer

Tool ini dibuat untuk tujuan edukasi. Pastikan mematuhi terms of service website dan gunakan secara bertanggung jawab.

---

## ğŸ¤ Contributing

Contributions welcome! Silakan buat issue atau pull request.

---

## ğŸ“ Support

Jika ada pertanyaan atau issue:
1. Check `docs/` folder untuk dokumentasi
2. Review logs untuk debugging
3. Adjust configuration sesuai kebutuhan

---

**Happy Scraping! ğŸš€**
